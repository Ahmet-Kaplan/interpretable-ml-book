## Detecting Concepts

`r if(is.html){only.in.html}`

<!--intro -->
<!-- This chapter presents techniques for analyzing which concepts a neural network learned.
Concept here means an abstract idea, which is pre-defined by a human.
While  [feature visualization](#feauture-visualization) tries to detect features from neural network units, which might match a concept (e.g. dog snouts) but doesn't have to, concept detection starts with a concept and analyzes how the neural network handles this concept. -->


<!-- why are concepts interesting? -->
<!-- Feature visualization is more explorational:
What does the neural network detect?
But it does not help when we have more concret questions, like how important was the concept of dog snouts for the classification? -->


<!-- Approaches that we will look at -->
<!-- We will look at two approaches: Network Dissection and Concept Activation Vectors.
Both approaches require additional labeling of data, but in different ways. -->

### TCAV: Testing with concept activation vectors.

#### Introduction

TCAV by Kim et al. (2019)[^tcav] is proposed to generate global explanations for neural networks in terms of user-defined concepts.
A concept can be literally anything: A color, a shape, or even an idea.
And TCAV measures the extent of a concept's influence on the model's prediction for a certain class.
For example, TCAV can answer questions such as how the concept of "striped" influences a model classifying an images as a zebra.
Since TCAV describes the relationship between a concept and a class, instead of explaining a single prediction, TCAV provides useful global interpretation for a model's overall behavior.

#### Intuition

Many methods we have encountered so far are explaining the behavior of a model in terms of its features.
There are two limitations of interpretability based on feature attribution: First, features are not necessarily user-friendly.
For example, the importance of a single pixel usually does not convey much meaningful interpretation.
Second, the expressiveness of feature-based explanations is constrained by the number of features.
Considering neural networks are capable of mapping the given features into a more expressive representation, we want to leverage that high-level representation to generate explanations that are more easily understood by humans.

#### Concept Activation Vector (CAV)

A CAV is simply the numerical representation that generalizes a concept in the activation space of a neural network layer.
A CAV, denoted as $v_l^C$, depends on a concept $C$ and a neural network layer $l$, where $l$ is also called as a bottleneck of the model.
For calculating the CAV of a concept $C$, first, we need to prepare two datasets: a concept dataset which represents $C$ and a random dataset that consists of arbitrary images.
For instance, to define the concept of "striped", we can collect images of striped objects as the concept dataset, while the random dataset is a group of random images without stripes.
Next, we target a hidden layer $l$ and train a binary classifier which separates the activations generated by the concept set from those generated by the random set.
The coefficients of this trained binary classifier is then the CAV of the concept.
Mathematically, CAV is a vector that is orthogonal to the decision boundary, pointing to the activation space of concept dataset.
In the TCAV Github source code, the authors provide SVM and logistic regression model as the options for the binary classifier.

```{r tcav, fig.cap="Figure from TCAV paper, Been Kim et. al (2018) ", out.width=800}
knitr::include_graphics("images/tcav.png")
```

Lastly, we can measure the “conceptual sensitivity” using the directional derivative:

$$S_{C,k,l}(x)=\nabla h_{l,k}(f_l(x))\cdot v_l^C$$

where $h_{l,k}$ is the logit output of the class $k$ for the input $x$.

#### Testing with CAVs (TCAV)

In the last paragraph, we learned how to calculate the conceptual sensitivity of a single data point.
However, our goal is to produce a global explanation that indicates an overall conceptual sensitivity of an entire class.
A very straightforward approach achieved by TCAV is to calculate the ratio of k-class inputs whose conceptual sensitivities $S_{C,k,l}$ are positive:

$$TCAV_{Q_{C,k,l}}=\frac{|{x\in X_k:S_{C,k,l}(x)>0}|}{|X_k|}$$

Going back to our example, we are interested in how the concept of "striped" influences the model while classifying images as "zebra".
We collect data that are labeled as "zebra" and calculate conceptual sensitivity for each imput image.
Then the TCAV score of $C$ on predicting "zebra" is the number of images who have positive conceptual sensitivities divided by the total number of the images labelled as "zebra".
In other words, a $TCAV_{Q_{"striped","zebra",l}}$ equal to 0.8 indicates that 80% of predictions for "zebra" class are positively influenced by the concept of "striped".

This looks great, but how do we know TCAV score is meaningful? After all, CAVs are trained by user-selected concept and random datasets. If datasets used to train CAVs are bad, CAVs can be misleading and useless. And thus, we perform a simple statistical significance test to help TCAV become more reliable. Simply, instead of training only one CAV, we train multiple CAVs using different random datasets. A meaningful concept should generate CAVs with consistent TCAV scores. More detailed test procedure is shown as the following:
- Collect $N$ random datasets, where it is recommended that $N$ is at least 100.
- Fix the concept dataset and calculate TCAV score using each of N random datasets.
- Apply a two-sided t-test to N TCAV scores against N TCAV scores generated by a random CAV.
A random CAV can be obtained by choosing a random dataset as the concept dataset.
It is also suggested to apply a multiple testing correction method here if you have multiple hypotheses.
The original paper uses Bonferroni correction, and here the number of hypotheses is equal to the number of concepts you are testing.

#### Advantages

Since users are only required to collect images for training the concepts that they are interested in, **TCAV does not require users to have machine learning expertise**.
This allows TCAV to be extremely useful for domain experts to evaluate their complicated neural network models.

Another unique characteristic of TCAV is its **customizability** enabled by TCAV's **explanations beyond feature attribution**.
Users are able to investigate any concept as long as the concept can be defined by its concept dataset.
In other words, a user can control the balance between complexity and interpretability based on their needs: If a domain expert understands the problem and concept very well, they can shape the concept dataset using more complicated images in order to generate a more fine-grained explanation.

Finally, TCAV generates **global explanations** that relate concepts to any class.
A global explanation gives you an idea that whether your overall model behaves properly or not, which usually cannot be done by local explanations. 
And thus TCAV can be used to identify "flaws" or "blindspots" that are occurred in a trained neural network: Maybe your model has learned to weight a concept too much or too less.
If a user can identify those ill-learned concepts, one can actually use the knowledge to **improve their model**.
Let's say there is a classifier that predicts "zebra" with a high accuracy. TCAV, however, shows that the classifier is more sensitive towards the concept of "dotted" instead of "striped".
This tells you that the classifier is accidentally trained by an unbalanced data, allowing you to learn to improve the model by either adding images of "striped zebra" to or removing images with "dotted zebra" from the training data.

#### Disadvantages

TCAV might **perform badly on shallower neural networks**.
As many papers suggested [^probe], deeper layers are more expressive and separable to learn knowledge representation.
If a network is too shallow, its layers may not be capable of separating concepts clearly.

Since TCAV requires **additional annotations** for concept datasets, it can be very expensive for tasks that do not have ready labelled data.

Although TCAV is hailed because of its customizability, it is **difficult to apply to concepts that too abstract or general**.
This is mainly due to the fact that TCAV describes a concept via the corresponding concept dataset.
The more abstract or general a concept is, such as "happiness", the more data are required to train a CAV for that concept.  

TCAV can be **computationally expensive**. A typical TCAV explanation is generated by running  TCAV calculation 500 trials, with training a CAV using 50 concept images and 50 random images during each trial. You may even want to increase the scale of training data and trials if the explanation is not statistically significant.

Though TCAV gains popularity in applying to image data, it has **limited applications in text data and tabular data**.

<!-- You have to learn the concepts from data.
That means if you want to understand whether the network uses the concept of "female" for the classification of e.g. images, you have to provide some examples of "female" (could be images with women in it), and non-female (images without women in it).

You send all those images through the network 

Good thing is that TCAV does not require to change the network you are using, but you can use the network that you already have.




TCAV uses directional derivatives to quantify the importance of a concept for the classification or prediction.
The concept is defined by the user and must be defined via some positive and negative data examples.
For example for the image classification of a zebra, the concept might be stripes.
The concept is defined byselecting images of stripes and some randomly sampled images without stripes.

Code for TCAV: https://github.com/tensorflow/tcav

TODO: CONTINUE DESCRIBING TCAV

Good things about TCAV:
The concepts are not required to be known at training time.
Really any concept can be analyzed, as long as you find some positive and negative examples. -->

<!-- Feature Visualization for RNNs -->
<!-- For RNNs: https://medium.com/@plusepsilon/visualizations-of-recurrent-neural-networks-c18f07779d56
https://distill.pub/2019/memorization-in-rnns/
http://lstm.seas.harvard.edu/

TODO: Checkout RNNVis and LSTMVis

List of notebooks:
https://github.com/tensorflow/lucid
More a tool for getting a general, better understanding of cnns, but not for daily job.

### Word Embeddings

**Word Embeddings**
Word embeddings represent words as vectors which can be used to compute the similarity between words.
As another way to visualize concepts that were learned are word embeddings.
An embedding maps a discrete feature (e.g. a word) to a m-dimensional vector.
A word embedding is the vector in some embedding space a word is mapped onto.
The embedding space is learned by the neural network.
The directions in that space often correlate to concepts.
This means that words with similar vectors have some similarity, e.g. cat and dog.
This also has the nice effect that we can do arithmetics in that space.
e.g.

$$embedding(king)-embedding(queen)=embedding(man)-embedding(woman)$$

The embeddings are high-dimensional vectors.
For visualization, they are often mapped to 2 Dimensions (e.g. with tSNE) TODO: CITE

What can you do with embeddings?
You can visualize the concepts that were learned.
Embedding let us analyze what the neural network learned.
For example, did it learn some kind of bias?
How do we get word embeddings?
Other use cases include to use these embeddings as feature transformations before the e.g. text is used in a machine learning model.

How are they created?
It's a mapping from categorical features (e.g. words) to some vectors.
They can be initialized with random weights and the embeddings are learned along with the thing you are trying to predict, e.g. with a recurrent neural network.
An alternativ is to use a pre-trained embedding like word2vec, GloVe or fasttext.
Those are trained over huge corpuses of text to predict words from their neighboring words.


 - concepts can transform when learning, e.g. dog into waterfall

**Detecting Concepts During Training Time**

Towards Robust Interpretability with Self-Explaining Neural Networks


**Software**

- CAffee and with GANS https://github.com/Evolving-AI-Lab/synthesizing


### Other approaches for concepts

- Word embeddings https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
-  -->

[^tcav]: Kim, Been, et al. "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)." arXiv preprint arXiv:1711.11279 (2017).

[^probe]: Alain, Guillaume, et al. "Understanding intermediate layers using linear classifier probes." arXiv preprint arXiv:1610.01644 (2018).

<!-- [^dissect]: Bau, David, et al. "Network dissection: Quantifying interpretability of deep visual representations." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. -->
