## Feature Attribution for Image Classification

`r if(is.html){only.in.html}`

<!--
Some literature
- For feature attribution: http://blog.qure.ai/notes/deep-learning-visualization-gradient-based-methods
- http://blog.qure.ai/notes/visualizing_deep_learning
- Also check out: https://openreview.net/pdf?id=Hkn7CBaTW
- https://www.youtube.com/watch?v=h9Y3FI_3lBw
- Good overview: TOWARDS BETTER UNDERSTANDING OF GRADIENT - BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS [^better-understanding], 2018
-->

<!--Idea for chapter:
- Start with general goal
- Intuition for how methods work
- Categorization of methods? [maybe later in chapter, after 3 approaches]
- Roughly explain 3 different approaches:
    - Vanilla Gradient: Because simple and educational.
    - Grad-CAM (or SmoothGRAD?) because it works
    - Some other which is not GradCAM and still works. Integrated Gradients? LRP?
- Application [VGG16, INNvestigate]
- Problems of attribution methods
- List of methods that are excluded because they don't work
- List of more methods
- Advantages / Disadvantages
- Software
-->

<!--
Questions
- How do those methods handle the RGB dimensionality? Average?
-->

<!-- short summary -->
Saliency maps are "heatmaps" that are often overlayed on images to highlight the pixels that were responsible for a certain neural network classification.
They are based on backpropagation of the gradient.
And they look a bit like this:

```{r, out.width = "80%"}
knitr::include_graphics("images/vanilla.png")
```

Other names that are used in this context: sensitivity map, saliency map, pixel attriution map, gradient-based attribution methods, feature relevance, feature attribution, feature contribution

<!-- General idea and distinction -->
Saliency maps are feature attribution methods for images that attribute the prediction to the input features.
Feature attribution explains individual predictions by attributing each input feature by how much it changed the prediction (negative or positive).
Be it input pixels, tabular data or words.
For saliency maps these are pixels.
These methods usually do not change the network, and we will only look at methods that are post-hoc, meaning they can be applied after the network was trained as usual.

A bit more formal definition would be this:
A feature attribution method of the prediction for p-dimensional input x, relative to some base input (or prediction?) can be expressed as a vector of relevances: $(r_1,\ldots,r_p)$.
The j-th element r~j~ is the contribution of the j-th feature input to the prediction.

There is a confusing amount of approaches out there to compute feature attributions.
We can get a handle on them by understanding two things: 1. Some methods manipulate parts of the image (model-agnostic) and some methods compute the gradient of the prediction (or classification score) with respect to the input features.
2. The gradient based methods (of which there are many) mostly differ in how the gradient is computed.

All methods have in common that in the end we get an output that has the same size as the input image (or can at least be projected meaningfully onto it) and gives each pixel a value which somehow can be interpreted as the relevance of the pixel for the prediction or classification of that image.

There are different ways to achieve these kind of attributions:

- Gradient based: Look at the gradients of the input pixels. Also called saliency. The other option are perturbation based, which is model-agnostic and by replacing parts of the image with some "neutral" image.
- The attribution might show only the positive  contributions towards the class of interest. Or it might show, in two different colors, both positive and negative. Or it might just show localization, which are regions that are important on an image
- Then there gradient vs. path attribution. The gradient just tells us whether a change of this pixel (increasing color) would change the prediction. The path attribution compares the current image with a reference image (which can be some artificial "Null" image) and distributes the difference of the prediction for both among the pixels. This article researches what is a good baseline.  Link: https://arxiv.org/pdf/1703.01365.pdf . Same as feature attribution, but with reference point instead.
[^unreliable-saliency] classify gradient based methods into 3 categories:
Gradients, Signal Methods (DeConvNet, Guided BackProb, PatternNet) and Attribution methods (Deep Taylor, Integrated Gradients).


For the pixels, attributions are averaged over the channels.

We only look at gradient-based methods here.
Gradient-based methods compute the gradient of the output with respect to the input features.
The interpretation of the gradient with respect to the input features is:
If I were to change this feature, the predicted class probability would go up (for positive gradient) or down (for negative gradient).
The larger the absolute value of the gradient, the stronger the effect of changes in that feature.

Question to myself:
Using the pure gradient should not make too much sense, because it does not matter how I change the pixel, only by the absolute change in prediction?

<!-- TODO: Explain the overall algorithm template here -->
The general recipe for the gradient-based attribution methods is simple:
All methods have one-forward pass to get the prediction and one backward pass (to get the gradients).
Then we know for each pixel the gradient.
But there is one obstacle, which is one reason why we have so many different approaches:
Neural network have non-linear transformation units, and therefore there is some ambiguity how to exactly do the backward pass of the gradients.
The difference to the normal backpropagation is that we compute the gradient with respect to the input features.
Backpropagagion compute the gradient with respect to the weights of the neural network.

More formally, we consider neural networks that output as prediction a vector of length $C$, which includes regression where $C=1$.
Output of DNN is called $S(x)=[S_1(x),\ldots,S_p(x)]$.
Formally all those methods take in input $x\in\mathbb{R}^p$ (can be image pixels, tabular data, words, ...) and outputs an explanation $R^c=[R_1^c,\ldots,R_p^c]$, one relevance value for each of the p input features.
The c indicicates the relevance for the c-th output.

In multi-class classification you have to decide for which classification to look at the relevance of the inputs.
This can be the correct class of that example, at least that's an interesting case to look at.
But also if DNN was wrong, it might be interesting to look at the input relevance for the wrong class and also for the relevance for the correct class for debugging.

The word attribution maps means that, for images, we visualize the pixels with red if they positively contributed, blue if negatively.
Of course, you are free to choose any color you like.

Some of those methods have the property of Completeness, meaning that the sum of the relevance values for all input features sum up to the difference between the prediction of the image/data point minus the prediction of a reference point (e.g. all grey image).
Integrated Gradient and SHAP have this property.

With all these approaches, we assume that we have an already trained CNN.

And we start with a rather simple approach, the "vanilla" gradient.

### Vanilla Gradient (Saliency Maps)

The idea of vanilla gradient, presented by [^saliency] as one of the first saliency approaches is quite simple, we compute the gradient of the loss function for the class we are interested in with respect to the input features.
This gives us a map of the size of the input features with negative to positive values.
Concerning naming, they called their approach "Image-Specific Class Saliency".

The main recipe for this approach is:

1. Do a forward pass of the image of interest
1. Do a backward propagation of the desired class to the input features to get the gradient
   $$E_{grad}(I_0)=\frac{\delta{}S_c}{\delta{}I}|_{I=I_0}$$
   Thereby we set all other classes to zero.
1. Visualize the gradients. Either absolute or with negative / positive

More formally, we have an image I and the CNN give it a score $S_c(I)$ for class c.
The score is a highly non-linear function of our image.
But the motivation is that we can approximate that score by applying first-order Taylor expansion

$$S_c(I)\approx{}w^T{}I+b,$$

where w is the derivate of our score:

$$w = \frac{\delta S_C}{\delta I}|_{I_0}$$

Now, there is some ambiguity how to do a backward pass of the gradients, as non-linear units such as ReLU (Rectifying Linear Unit) "remove" the sign.
So when we do a backpass, we don't know whether to assign a positive or negative or negative activation.
Using my incredible ASCII art skill, the ReLU function looks like this "_/" and is defined as $X_{n+1}(x)=max(0,X_n)$ from layer $X_n$ to layer $X_{n-1}$.
This means that when the activation of a neuron is 0, we do not know what to backpropagate.
In the case of vanilla gradient approach, the ambiguity is resolved as:

$$\frac{\delta f}{\delta X_n} = \frac{\delta f}{\delta X_{n+1}} \cdot \mathbb{1}(X_n > 0)$

where $\mathbb{1}$ is the element-wise indicator function which is 0 where the activation at the lower layer was negative or zero, and 1 where it is positive.
Or, in words, we take the gradient that we have back-propagated so far until the layer n+1, then simply set the gradients to zero where the activation at the layer below is not positive.

Let's assume we have layer $X_n$ and $X_{n+1}=ReLU(X_{n+1})$. 
Our fictive activation at $X_n$:

$$
\begin{pmatrix}
1 & 0 \\
-1 & -10 \\
\end{pmatrix}
$$

And these are our gradient at $X_(n+1)$:

$$
\begin{pmatrix}
0.4 & 1.1 \\
-0.5 & -0.1  \\
\end{pmatrix}
$$

The our gradients at $X_n$ will be:

$$
\begin{pmatrix}
0.4 & 0 \\
 0 & 0  \\
\end{pmatrix}
$$



#### Problems with Vanilla Gradient

Vanilla Gradient has Saturation Problem (explained in [^deeplift]):
Assuming ReLU activation layer is used, when the weighted feature sum goes below zero, the at some point the activation is capped at zero, but does not change any more, i.e. the activation is saturated.
Let's say the input to the layer are two neurons with weights $-1$ and $-1$ and a bias of $1$.
Passing that through the ReLU, the activation will be neuron 1 + neuron 2 when the sum of both neurons is smaller than 1.
If the sum of both is greater than one, the activation remains sasturated at an activation of 1.
Also the gradient at that point will be zero, i.e. all gradient based methods will show that this neuron is not important.

And now my friends, you get to understand another method, more or less for free: DeconvNet

### DeconvNet

DeconvNet by [^deconvnet] is almost identical to Vanilla Gradient.
A DeconvNet kind of uses the same components as a Convolutional Neural Network, but reverses the operations such as filtering, pooling and activation.
Apart from the ReLU layer, DeconvNet is equivalent to the Vanilla Gradient approach.
Vanilla Gradient can be seen as a generalization of DeconvNet.
For ReLU ($X_{n+1}=max(X_n,0)$) the vanilla gradient computes: $\delta{}f/\delta{}X_n=\delta{}f/\delta{}X_{n+1}\mathbb{1}(X_n>0)$, where $\mathbb{1}$ is an (element-wise) indicator function.
For DeconvNet ReLU, this is instead: $R_n=R_{n+1}\mathbb{1}(R_{n+1}>0)$, where $R_n$ and $R_{n+1}$ are the layer reconstructions.
When doing the backpass from layer x to layer x-1, vanilla gradient "remembers" which of the layer x were set to zero in the forward pass and sets those to 0 in layer x-1.
In DeConvNet, activation with negative value in layer x are set to zero in layer x-1.
Here layers are just the activations before and after ReLU.
Guided Backpropagation sets both to 0 in layer x-1: neurons that in the forward pass were 0 in layer x and neurons that are negative in the backward passe in layer x.
Both DeConvNet and Guided Backpropagation do not compute the true gradient but an imputed one.
If we continue our example from above, then the gradient $X_n$ (they call it layer reconstruction) becomes:

$$
\begin{pmatrix}
0.4 & 1.1 \\
0 & 0  \\
\end{pmatrix}
$$



### Grad-CAM

Grad-CAM provides visual explanations for CNN decisions.
In contrast to other methods, the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer to produce a rough localization map that highlights important regions of the image.

Grad-CAM stands for Gradient-weighted Class Activation Map.
And, as the name suggest it is based on the gradient of the neural networks.
Grad-CAM, like other techniques, assigns some importance to each neuron, for the decision of interest.
This is the class prediction (which we find in the output layer), but can, in theory, be any other layer in the neural network.
Grad-CAM backpropagates this information to the last convolutional layer.
Grad-CAM can be used with different CNNs: with fully-connected layers, for structured output such as captioning and in multi-task outputs and for reinforcement learning.
The great thing about Grad-CAM is that besides image classification it works also for other image related tasks.
This includes visual question answering and reinforcement learning.


<!-- An intuitive explanation -->
Let us start with an intuitive look at Grad-CAM.
The goal of Grad-CAM is to understand at which parts of an image a convolutional layer "looks" for a certain classification.
As a reminder, the first convolutional layer of a CNN takes as input the images and outputs feature maps, which encode learned features (see [chapter Learne Features](#cnn-features)).
The higher-up convolutional layers do the same, but take as input the feature maps of the convolutional layer before.
To understand how the CNN makes decisions, Grad-CAM analyzes where in the feature maps of the last convolutional layers relevant stuff happens.
There are k feature maps in the last conv-layer, and I will call them $A_1, A_2, \ldots, A_k$.
How can we "see" from the feature maps, how the CNN made a certain classification?
First approach, we could simply visualize the raw values of each feature map, average this over the feature maps and overlay it on our images.
This would not be helpful, since the feature maps encode information for **all classes**, but we are interested in a certain class.
This means Grad-CAM has to decide how important each of the k feature map was for our class c that we are interested in.
This means we weight each pixel of each feature map by the gradient.
Then we average the feature maps pixel-wise weighted by the gradient.
This give us a heatmap which highlights regions that positively or negatively affect the class of interest.
This heatmap is send through the ReLU function, which sets all negative values to zero.
We remove all negative values, because we are only interested in the parts that contribute towards the chosen class c and not to the ones to other classes.
Since we are only interest in our chosen class, they suggest to look only at the positive gradients.
The word pixel here might be misleading, as the feature map is smaller than the image (because of the pooling units), but can later be mapped back to the region on the image.
The we scale the Grad-CAM map to be between 0 and 1 for visualization purposes and overlay it over the original image.


<!-- Pseudo code-->
Here again, in Pseudo-Code and with some math.
Our goal is to find the localization map defined as:
$$L^c_{Grad-CAM} \in \mathbb{R}^{uxv} = \underbrace{ReLU}_{\text{Pick positive values}}\left(\sum_{k} \alpha_k^c A^k\right)$$


where u is the width, v the height and c the class of interest.

1. Forward propagate the input image through the CNN
1. Obtain raw score for the class of interest, meaning the activation of the neuron before the softmax layer
1. Set all other class activations to zero 
1. Backpropagate the gradient of the class of interest to the last convolutional layer before the fully connected layers $\frac{\delta{}y^c}{\delta{}A^k}$
1. Weight each feature map pixel by the gradient for the class. Index i and j refere to the width and height dimensions:

   $$\alpha_k^c = \overbrace{\frac{1}{Z}\sum_{i}\sum_{j}}^{global average pooling} \underline{\frac{\delta y^c}{\delta A_{ij}^k}}_{gradients via backprop}$$

   This means that the gradients are global-average-pooled.
1. Compute an average of the feature maps, weighted per pixel by the gradient
1. Apply ReLU on averaged feature map
1. For visualization: Scale values to interval between 0 and 1. Upscale image size and overlay on original image.
1. Additional step for Guided Gra-CAM: Multiply heatmap with guided backpropagation


The Grad-CAM algorithm can be found here: https://keras.io/examples/vision/grad_cam/
<!-- Algorithm in Detail-->



Problems with Grad-CAM:


Problem of zeroing out negative gradients during backpropagation.

### Guided GradCAM

From the description of Grad-CAM you can see that the localization is very coarse, since the last convolutional feature maps have a much coarser resolution compared to the input image.
Other attribution techniques back-propagate the entire way back to the image and each of the pixels.
They are therefore much more detailed and can really show you individual edges or spots that contributed most to a prediction.
A fusion of both methods is called Guided Grad-CAM.
And it is super simple.
You compute for an image both the Grad-CAM output and the output from, for example, Deconvolution or some other pixel-wise attribution.
The Grad-CAM output is then upsampled with bilinear interpoliation, then both maps are multiplied element-wise.
Grad-CAM works like a lense that focuses on certain parts of the pixel-wise attribution map.

TODO: Add image

Problem of zeroing out negative gradients during backpropagation.

TODO: CONTINUE HERE
TODO: Move Smoothgrad below DeconvNet

### SmoothGrad

SmoothGrad can be seen as an extension of any of the methods, rather than its own method.
The paper for this method is titled "SmoothGrad: removing noise by adding noise" [^smoothgrad].
So the goal with SmoothGrad is to remove noise.
The idea is to take an image, generate more images by adding noise to this image, and then take the average of the resulting sensitivty maps as an explanation.
Another way to integrate over noise is to add noise already at training time.
And the paper shows that a combination of the two noise approches works best.

Why should this work?
The theory is that the derivative has great fluctuations at small scales.
The neural network did not have an incentive during training to keep those derivatives smooth.

Goal: reduce noise and visual diffusion.
Average over explanations.

$$E_{sg}(x)=\frac{1}{N}\sum_{i=1}^N{}E(x+g_i)$$

where $g_i\sim{}N(0,\sigma^2)$ are sampled noise vector.

Sigma should be set to a very low number like XXXX. TODO

### Alternative: Perturbation

Shapley, Occlusion, LIME for images

Occlusion, Shapley Value, LIME, wiggle around the inputs, visualize the directions in which the output changes
Problems with perturbation or occlusion: Network was never trained on those images, you are leaving the space of realistic images.
Might not be relevant how the neural network behaves here.


They are more expensive because multiple passes through the network are needed.
i.e. multiple predictions because many different version of the image are passed through.

What is the baseline that is chosen?
often the all black image, which has a prediction near zero.
For LIME image it is the average of each RGB

Shapley Value is covered in the [#shap](Shap Chapter) and LIME in the [#lime](LIME Chapter).



### Problems
<!-- move this chapter after the method explanations -->
There are too many methods.
And no clear path to check how they are doing, right?
I mean we would need some groundtruth to really say if they are correct?
Well, there is a different way, proposed as sanity checks for Saliency Maps [^sanity-checks].
The rough idea is: We cannot know whether a saliency map is correct, but we can know if a method is insensitive to model and data, which is highly undesirable.

As many methods were proposed, their evaluation was mostly visual, i.e. does the highlighting of pixels make sense?
The paper above brings some rigor to evaluation.
They propose randomization tests for model and data to evaluate the sensitivity of the feature attribution approaches.
The conclusions are quite striking.
Many methods (which are widely used) are independent of both training data and the model parameters.
That is bad news and means those methods should not be used.
Model parameter randomization test: compare output of saliency method of model with the same output when the network gets random weights (but same architecture).
Expectation: Quite different results.
If results are similar, it means that the method is not connected to the model (bad).
Data randomization test: Compare saliency maps of two trained networks.
First network trained on training data, second network on training data but with shuffled data.
Again, we would expect that the saliency maps differ quite a lot.
If not, the method is broken.
Theses are very basic checks.
If a method passes these checks, it could still be a wrong attribution method.
But if it fails these checks, the method is definitely broken and should not be used.


Problems with guided backpropagaion and similar methods:
Images closely resemble the output of edge detectors.
Edge detectors are independent of the model and the data, so this is really bad for guided backpropagation.
Gradients and GradCAM passed the sanity checks (model and data randomization).
Guided BackProp & Guided GradCAM failed.
Other cases (gradient \* input, integrated gradients, SmoothGrad) where not clear cut.

Also very dependent on the activation function that was used (ReLU, Tanh, softmax, ...)

Unfortunately, many of those methods have issues.
In an experiment, where the labels were mixed and the model was retrained, the explanations were stil very similar.
Only GradCAM was okay.

Feature attribution might also be called saliency maps.
There are lots and lots of approaches to to this, all very similar.
/We will only look at the general idea, and point out to some of the approaches.


### List of problems

Before we start with all the approaches, let's look at problems.
I know, a bit unusual, but it helps to weed out some of the approaches.
Some are also more "axiomatic" in the sense that these are problems to things that are desirable.

- Saturated Gradient (explained in [^deeplift]).
  Caused by the activation function.
  The flat part of an activation function (e.g. ReLU) does not change at some value.
  So looking at the gradient of a pixel, it might be zero, even though it contributed a lot, but the gradient is already saturated.
- Disconinuities in the gradients, also called the Thresholding artifact. Stems from jump in ReLU.
  Exists in Gradient and Gradient x Input
- Insensitivity to Model
- Insensitivity to Training Data
- Failure to highlight negative contributions (due to backpropagation through ReLUs)
- Input sensitivity failure: If input and baseline differ in one feature and have different predictions, the relevance of that feature has to be non-zero. Some methods fail this [^integrated-gradient].
  Lack of input sensitivity leads to focus on irrelevant features.
  Gradient saturation causes input sensitivity failure for DeConvNets and Guided Backpropagation.
- Failure of Implementation Invariance: An attribution should be identical when two networks give exactly the same predictions for any inpt.
  This means no matter what the network does internally, as long as the output remains the same, so should the attributions to the pixels.
  If an attribution method suffers from failure of implementation invariance, it may attribute to irrelevant inputs.
This happens because DeepLift (and also LRP) replace gradients (which are implementation invariant) with discrete gradients (i.e. finite sums) plus a modified form of backpropagation.-




### Examples


I want to predict the following:
TODO: All the images here
Most likely class Italian_greyhound (35.21%)

Neural Network VGG16 TODO: CITE

```{r, out.width = "80%"}
knitr::include_graphics("images/smoothgrad.png")
```

TODO: Insert image with edge detectors



### Some more methods:

VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS:
- Based on Shapley Value for explaining individual predictions
- Relevance of feature is estimated by measuring how the prediction changes if feature is unknown, by simulating that the feature is unknown.
- what this paper does differently: instead of simulating
- Implementation: https://github.com/lmzintgraf/DeepVis-PredDiff
- DeepSHAP (gradient-based)
  - version called DeepExplainer. There is a connection between SHAP and DeepLift
  - version called GradientExplainer. Connection between SHAP and Gradient Input algorithm.
- Occlusion (perturbation based) https://arxiv.org/abs/1311.2901
- (epsilon) Layer-Wise Relevance Propagation (gradient-based)
- Gradient * Input (gradient-based, surprise!) [^integrated-gradients] https://arxiv.org/abs/1605.01713
- Shapley Value Sampling (perturbation based)
- LIME (perturbation based)
- Grad-CAM class activation maps (gradient-based) [^grad-cam]
- Guided Backpropagagion
- Smoothgrad [^smoothgrad]  https://arxiv.org/abs/1706.03825
- Deep Inside: Simonyan, K.; Vedaldi, A.; Zisserman, A. Deep inside convolutional networks: Visualising image classificationmodels and saliency maps.arXiv preprint arXiv:1312.60342013.
- All-CNN: Springenberg, J.; Dosovitskiy, A.; Brox, T.; Riedmiller, M. Striving for Simplicity: The All Convolutional Net.ICLR (workshop track), 2015.
- Deep Visualization: Yosinski,  J.;  Clune,  J.;  Nguyen,  A.;  Fuchs,  T.;  Lipson,  H.   Understanding neural networks through deepvisualization.arXiv preprint arXiv:1506.065792015
- VBP: Bojarski, M.; Choromanska, A.; Choromanski, K.; Firner, B.; Jackel, L.; Muller, U.; Zieba, K. Visualbackprop:visualizing cnns for autonomous driving.arXiv preprint arXiv:1611.054182016.
- Meaningful: Fong, R.C.; Vedaldi, A. Interpretable explanations of black boxes by meaningful perturbation.  Proceedings ofthe IEEE International Conference on Computer Vision, 2017.
  - code for above: https://github.com/jacobgil/pytorch-explain-black-box
- Grad-CAM++: Chattopadhyay, A.; Sarkar, A.; Howlader, P.; Balasubramanian, V. Grad-CAM++: Generalized Gradient-basedVisual Explanations for Deep Convolutional Networks2017.

### Further Papers analyzing saliency methods

- https://arxiv.org/pdf/1901.09392.pdf
- https://arxiv.org/pdf/1912.01451.pdf this paper says that the methods to analyze saliency maps is itself not very reliable


### Advantages
- The explanations are visual and easy to understand.
  Especially when methods only highlight important pixels, it becomes easy to understand that these were the most important regions fo the image.
- Methods that rely on gradient are usually faster to compute than model-agnostic methods.
  For example, [LIME](#lime) and [SHAP](#shap) can also be used, but are more expensive to compute.
- There are many methods to choose from.

### Disadvantages
- As with most interpretation methods, it is difficult to know whether the explanation is **correct**.
- Saliency maps can be very fragile.
  [^fragile-saliency] showed that introducing small (adversarial) perturbations to an image, that still lead to the same prediction can lead to very different pixels that are highlighted as explanation.
  They looked at DeepLift, Vanilla (Simple) Gradient and Integrated Gradients.
- The amount of methods is also a problem for practice.
  You have to see which methods works best for which network.
  But again, since we have no good way of showing whether it is correct, it is a difficult task to decide for a method.
  I mean, you can look at some explanations, but when they don't make sense, maybe it is just because the model works in weird ways.
- Some methods have similarities to edge detectors, which are independent of training data and model.
  Explanation becomes misleading.
- Also [^unreliable-saliency] showed that these methods can be highly unreliable.
  They added a constant shift to the input data, i.e. they added the same pixel changes to all images.
  They compare two networks, original network and the one were the bias of the first layer is changes to adapt for the constant pixel shift, so that the activations of both networks are the same and so are the predictions.
  Also the gradient is the same for both.
  This means network 2 simply cancels out the constant shift in the first layer.
- It's rather difficult to know how well these methods work.
  Even for the work that tries to measure how well they work, there is some criticism that these evaluation methods dont work well ...
- All in all, it is a very unsatisfying state of affairs.
  We need to wait a bit longer for more research on this matter.


### Software

There are various implementations of feature attribution methods.
For the example, I used [tf-keras-vis](https://pypi.org/project/tf-keras-vis/).
One of the most comprehensive libraries is [iNNvestigate](https://github.com/albermax/innvestigate), which implements vanilla gradient, smoothgrad, Deconvnet, Guided Backpropagation, PatternNet, LRP, ...
A lot of the methods are implemented in the DeepExplain Toolbox: https://github.com/marcoancona/DeepExplain



[^deeplift]: Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.

[^integrated-gradients]: Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. "Axiomatic attribution for deep networks." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.

[^saliency]: Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. "Deep inside convolutional networks: Visualising image classification models and saliency maps." arXiv preprint arXiv:1312.6034 (2013).

[^grad-cam]: Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE international conference on computer vision. 2017.

[^sanity-checks]: Adebayo, Julius, et al. "Sanity checks for saliency maps." Advances in Neural Information Processing Systems. 2018.

[^smoothgrad]: Smilkov, Daniel, et al. "Smoothgrad: removing noise by adding noise." arXiv preprint arXiv:1706.03825 (2017).

[^deconvnet]: Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." European conference on computer vision. Springer, Cham, 2014.

[^guided-backpropagation]: Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv preprint arXiv:1412.6806 (2014).

[^lrp] Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015).


<!-- References about problems -->

[^better-understanding]: Ancona, Marco, et al. "Towards better understanding of gradient-based attribution methods for deep neural networks." arXiv preprint arXiv:1711.06104 (2017).

[^fragile-saliency]: Ghorbani, Amirata, Abubakar Abid, and James Zou. "Interpretation of neural networks is fragile." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.

[^unreliable-saliency]: Kindermans, Pieter-Jan, et al. "The (un) reliability of saliency methods." Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, Cham, 2019. 267-280. https://arxiv.org/pdf/1711.00867v1.pdf

[^perplexing-behavior]: Nie, Weili, Yang Zhang, and Ankit Patel. "A theoretical explanation for perplexing behaviors of backpropagation-based visualizations." arXiv preprint arXiv:1805.07039 (2018).

<!-- Toolboxes -->

[^innvestigate]: "iNNvestigate neural networks!"(http://arxiv.org/abs/1808.04260) by Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam Hägele, Kristof T. Schütt, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller, Sven Dähne, Pieter-Jan Kindermans


<!-- not used


### DeconvNet, Deconvolution 2013-11-12

Paper: Visualizing and Understanding Convolutional Networks
By: Matthew D Zeiler, Rob Fergus
Date: 2013-11-12

The authors propose a Deconvolutional Network (DeconvNet).
A DeconvNet kind of uses the same components as a Convolutional Neural Network, but reverses the operations such as filtering, pooling and activation.
These nets have already been suggested two years earlier, but not for the purpose of visualization.

We start with an already trained neural network.
For a given image, we compute the all the feature maps.
The approach constructs a second neural network that covers the convolutional layers, but in reverse.
We start with the last convolutional layer (the feature maps) and go back until a reconstruction of the input image.
Pooling layers are reversed by unpooling:
Although this unpooling is not reversible (since max-pooling loses information), it is possible to obtain an approximation.
This works by recording the location of the maxima within each pooling region.
They call those "switch" variables.
In the DeconvNet unpooling, these switches are used to map the reconstructions from the layer above to the maximally activated location below.
The activation layer -- here Rectifying Linear Unit (ReLU) -- again uses ReLUs to backpropagate the reconstructions.

The filtering units are simply transponsed, but applied to the rectified maps, not the output layer beneath.
[On this github README](https://github.com/vdumoulin/conv_arithmetic) you can see some animations how this looks like.

We do not cover too deeply here, because it has some problems and the following, more simple approaches work better.

[^perplexing-behavior] showed that Guided Backpropagation and DeConvNet do partial image reconstruction, which is undesirable.
They also have a nice image explaining the difference between backpropagation of activation between saliency, deconvnet and guided backpropagation.

DeConvNet uses a "Backward ReLU" operation.

DeConvNet suffers from
- Input sensitivity failure: Reason is that methods that back-propagated through ReLU only if ReLU is "turned on" for this input. I guess same issue as gradient saturation.


DeConvNet can also be used for [Feature Visualization](#feature-visualization).
DeConvNet is like vanilla gradient, except of the behavior at the ReLU units.


### DeepLIFT

Paper: Learning Important Features Through Propagating Activation Differences
Authors: Avanti Shrikumar, Peyton Greenside, Anshul Kundaje
Date: 2019-10-12 (arxiv, v2)
Link: Learning important features through propagating activation diffe    rences
[^deeplift]

Also backpropagation based approach.
The name DeepLIFT stands for Deep Learning Important FeaTures.
DeepLIFT attributes the difference in prediction from some reference point prediction among all features.
DeepLIFT explains a prediction as the difference from the prediction of some reference point.
This reference is a neutral or default input.
The choice depends on the task.
For an image this can be all black pixels.

Notation:

- $c$ is class of interest
- $x$ is vector of input pixels
- $x_0$ is reference image pixels
- $f$ is the neural network prediction or score just before classification layer (before softmax)
- $f(x)$ is predictions
- $f(x_0)$ is prediction of reference image
- $\Delta{}f=f(x)-f(x_0)$ is difference in prediction to reference image
- $C_j$ is attribution assigned to the $j$-th pixel $x_j$


DeepLIFT assigns scores to each input feature.
Explained is the difference between some prediction $\Delta{}f=f(x)-f(x_0)$.

$$\sum_{j=1}^p{}C_j$$

where $C$ is the contribution of a feature.

DeepLift works by a set of rules that are applied to calculate the individual contributions.

$$\Delta{}y=\Delta{}y^++\Delta{}y^-$$

For dense and convolutional layers, the postive parts and negative parts of an activation are described with:

$$\Delta{}y^+=\sum_{i}\mathbb{1}\{w_i\Delta{}x_i>0\}w_i\Delta{}x_i=\sum_{i}\mathbb{1}\{w_i\Delta{}x_i>0\}w_i(\Delta{}\x_i^{+}+\Delta{}x_i^{-})$$
$$\Delta{}y^-=\sum_{i}\mathbb{1}\{w_i\Delta{}x_i<0\}w_i\Delta{}x_i=\sum_{i}\mathbb{1}\{w_i\Delta{}x_i<0\}w_i(\Delta{}\x_i^{+}+\Delta{}x_i^{-})$$

Contributions are calculated based on these differences (called "Linear Rule")
$$C_{\Delta{}x_i^+\Delta{}y_i^+}=\mathbb{1}\{w_i\Delta{}x_i>0\}w_i\Delta{}x_i^+$$
$$C_{\Delta{}x_i^-\Delta{}y_i^+}=\mathbb{1}\{w_i\Delta{}x_i>0\}w_i\Delta{}x_i^-$$
$$C_{\Delta{}x_i^+\Delta{}y_i^-}=\mathbb{1}\{w_i\Delta{}x_i<0\}w_i\Delta{}x_i^+$$
$$C_{\Delta{}x_i^-\Delta{}y_i^-}=\mathbb{1}\{w_i\Delta{}x_i<0\}w_i\Delta{}x_i^-$$

where $y$ is the activation before the activation layer.
Nonlinear activation functions such as ReLU, tanh and sigmoid are processed with the following rule:

$$m_{\Delta{}x\Delta{}y}=\frac{\Delta{}y}{\Delta{}x}$$

This rescale rule only addresses saturation and thresholding problems, but not the min/AND rule.

So, alternatively can use the RevealCancel Rule, which addresses all three.
But this rule can be sensitive to noise, so the ReScale rule can sometimes be preferable (see https://www.youtube.com/watch?v=f_iAM0NPwnM)

No distinction made between positive and negative deltas.


There is a connection between DeepLIFT and Shapley VAlues.
DeepLIFT is an approximation of Shapley Values but with reference point.

How to choose reference.
Either there is a natural reference, such as all zeros for MNIST (is the background color).
Or one can use multiple references and average results.


DeepLift is Shapley Value approximation.
Shapley where reference point is "not participating team"


DeepLift breaks implementation invariance [^integrated-gradients].
This happens because DeepLift (and also LRP) replace gradients (which are implementation invariant) with discrete gradients (i.e. finite sums) plus a modified form of backpropagation.
For normal backpropagation, the implementation invariance comes from $\delta{}f/\delta{}x$ could be computed directly, without the chain-rule, in theory, and mathematically has to yield the same result with when using the chain-rule.
The chain rule uses implementation details (i.e. intermediate layers), but will come to the same result in the end, which is the definition of implementation invariance.
The chain rule does not hold for discrete gradients and thus implementation invariance breaks.

Deep Lift and $\epsilon$-LRP can both be re-formulated as computing backpropagation for modified gradient function
Ancona et. al 2018.
Some tips and tricks (for LRP): Methods for Interpreting and Understanding Deep Neural Networks
LRP: Should work better on ReLU


These gradient based methods are all different for different activation functions, since when the chain rule for derivation is applied, they replace the non-linear activations with a function $g(x)$ is different in different methods.


### Deep Taylor

Deep Taylor is a reference method, i.e. attributions are relative to some reference data point.

First, the attribution of an input neuron j is initialized to be equal to the output of taht neuron.
iAttribution of other is set to zero. $s_j^{output}=y$, $s^{output}_{k\neqj}=0$

The attribution then is backpropagated to inputs with the following rule:

$$s^{l-1}_j=\frac{w\cdot(x-x_0)}{w^Tx}s_j^l$$

where $x_0$ is the pixel vector of the reference image.


When constant is added to image, does not work any more, depending on choice of reference point [^unreliable-saliency]
Deep Taylor is equivalent to LRP, when an all zero reference point is chosen.
When all zero is chosen, they method is sensitive to constant shift in image -> bad

### Integrated Gradients  2017-06-13

Citation: [^integrated-gradients]
Paper: Axiomatic Attribution for Deep Networks
Link: https://arxiv.org/pdf/1703.01365.pdf


Integrated gradients, informally, can be seen as combininig gradients with the path dependent approaches such as LRP and DeepLIFT.
The basic idea: Choose some baseline input (e.g. black image), consider a straight line between baseline and actual image (in the pixel space), and integrated the gradients when moving the pixel colors from one image to the other.
It's the path integral.
If we had a two pixel image [0.1, 0.2] and a baseline image [0,0], intermediate steps would be [0.7, 1.4] and [0.01, 0.02] , when moving from image to baseline.

They defined some desiderata, which, of course, Integrated Gradients all fulfil:
- Sensitivity: If input and baseline differ in one feature and have different predictions, the relevance of that feature has to be non-zero.
I think that is a fair requirement.
Gradient alone does not fulfill this axiom, because gradient can be zero, but feature different.
Also deconvnet and guided backpropagation break sensitivity.
- Implementation Invariance: For two networks that have exactly the same predictions, no matter how the input looks like, the attribution should be the same.
Even if the networks work differently inside.
I think this is also a fair assumption.
LRP and DeepLift don't satisfy implementation invariance.
If methods don't have implementation invariance, they are senstive to unimportant workings of the network.
- Completeness: The attributions / relevance score add up to the difference between input x and the chosen baseline.
Integrated Gradients, DeepList and LRRP do so.
- Dummy: if  a network does not depend on a feature at all, its relevance should be zero.
- Linearity: If we linearly combine two networks (e.g. weighted sum of the prediction of both), then the attribution for a feature should also be a weighted sum (with same weights as in linear combination).
- Symmetry: swapping two features should yield same attribution (see shapley value)
  It's important to do the linear interpolation to achieve symmetry.
  There are infinite path one could take, e.g. first fully change one pixel, than the next and so on.
  Each path would lead to a different attribution.
  Integrated Gradients chose the linear path.

(most axioms are like their shapley equivalents)


Integrated gradients correspond to a cost-sharing method called Aumann-Shapley.

$$E_{IG}=(x-x_0)\cdot\int_0^1\frac{\delta{}f(x_0+\alpha(x-x_0)}{\delta{}x}$$

Here $x_0$ is a reference image, that stands for absence of features.
Usually a black or otherwise one-colored image.
IG is a reference point method, i.e. they compare to a reference.
IG is sensitive to constant shift [^unreliable-saliency] which is bad, but only when an all zero reference point is chosen.
When a black image is chosen as reference, it is fine (set all pixels to the mininimum in the data).

Numerical integral is, of course, some computation overhead compared to other methods.

They are computed using summation.
We cut the path into $m$ parts replace integration with the sum (which is called the Riemman approximation of integrals):

$$E_{IG}=(x-x_0)\cdot\sum_{k=1}^m\frac{\delta{}f(x_0+\frac{k}{m}\cdot(x-x_0)}{\delta{}x}\cdot\frac{1}{m}$$


Integrated Gradients address the saturation and thresholding problems.
Has a reference point.
Can still give misguiding results, according to DeepLift paper.
https://distill.pub/2020/attribution-baselines/

Integrated Gradients are motivated by Aumann-Shapley value from cooperative game theory.

You can also average contributions over many different reference point, a reference distribution.


### Guided Backpropagation 2015-04-13

Paper: Striving for Simplicity: The all Convolutional Net
Date: 2015-04-13
Link: https://arxiv.org/pdf/1412.6806.pdf


The paper proposes CNN architecture solely on convoluation layers without maxpooling layers.
But also they introduce a new variant of DeConvNet.
Combines Vanilla Gradient and Backpropagation?
Guided Backpropagation computes grradients, but any gradient that becomes negative during the backward pass is discarded at the ReLUs (i.e. set to zero).

Guided Backpropagation break Input Sensitivity (and Gradient Saturation?). [^integrated-gradients].

Builds on Deconvolution.
Negative gradients are set to zero.
Gradient is backpropagated through backward ReLU, then forward ReLU.

[^perplexing-behavior] showed that Guided Backpropagation and DeConvNet do partial image reconstruction, which is undesirable.

By combining VAnilla Gradient and DeConvNet we end up with more zero attributions.
Like Deconvnet, Guided Backpropagation can also be used for [Feature Visualization](#feature-visualization).

![](images/backpass.png)

### Layerwise Relevance Propagation LRP 2016

Paper: On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation
Authors: Alexander Binder, Sebastian Bach, Gregoire Montavon, Klaus-Robert Müller, and Wojciech Samek
Date: 2015
Link: http://iphome.hhi.de/samek/pdf/BinICISA16.pdf
[^lrp]



First, you do a forward pass of the image to get the prediction.
Then, you do a backward pass and at each layer, redistribute the outcome among the layer below:

Idea: $f(x)\approx\sum_{i=1}^pR_i$

where each contribution $R_i$ belongs to an input pixel.
Contributions can be positive or negative.

$$R_i^{(l)}=\sum_{j}\frac{z_{ij}}{\sum_{i'}z_{i'j}}R_j^{(l+1)}$$

with $z_{ij}=x_i^{(l)}w_{ij}^{(l,l+1)}$.

where $i$ is the index of a neuron at layer $l$.
The $j$ index runs over all upper-layer neurons, to which neuron $i$ contributes.
Using this satisfies the efficiency property: $\sum_pR_p^{(1)}=f(x)$.
In their paper, they call it conservation property.

They introduce two more variants, LRP-$\beta$ and LRP-$\epsilon$, which relax the efficiency property to get some better numerical properties.


LRP for ReLU networks is equivalent -- up to some factor -- to Gradient \* Input.
And Gradient x Input does not address the saturation problem.


LRP breaks implementation invariance [^integrated-gradients].

### Gradient \* Input, DeepLift, $\epsilon$-LRP

It is the same as the gradient method, but multiplied with the value of the feature / pixel.
Helps with the problem of gradient saturation and reduces visual diffusion, as can be seen in the examples later.

$$E_{GI}=x\cdot\frac{\delta{}f}{\delta{}x}|x=x_0$$

All three methods are equivalent, as shown by M. Anaconda (in ReLU networks, no bias, zero baseline).

When constant is added to image, does not work any more. [^unreliable-saliency]

-->
