```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
```

# Local Model-Agnostic Methods {#local-methods}

Local interpretation methods explain individual predictions.
You will learn about the following local explanation techniques in this chapter:

* [Individual conditional expectation curves](#ice) are the building blocks for [partial dependence plots](#pdp) and describe how changing a feature changes the prediction.
* [Local surrogate models (LIME)](#lime) explains a prediction by replacing the complex model with a local interpretable surrogate model.
* [Scoped rules (anchors)](#anchors) are rules that describe which feature values anchor a prediction, in the sense that they fixate the prediction.
* [Counterfactual explanations](#counterfactual) explain a prediction by studying which features should be changed to reach a desired change in prediction.
* [Shapley values](#shapley) is an attribution method that fairly attributes the prediction to individual features.
* [SHAP](#shap) is a different computation method for Shapley values, but also proposes global interpretation based on combinations of Shapley values over the data.

LIME and Shapley values are attribution methods, so that the prediction of an individual instance is described as the sum of feature effects.
Other methods, such as [counteractual explanations](#counterfactual) are example-based.


