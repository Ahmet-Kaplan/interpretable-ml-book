
<!--{pagebreak}-->

## Counterfactual Explanations {#counterfactual}

```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

A counterfactual explanation describes a causal situation in the form: "If X had not occurred, Y would not have occurred".
For example: "If I hadn't taken a sip of this hot coffee, I wouldn't have burned my tongue".
Event Y is that I burned my tongue;
cause X is that I had a hot coffee.
Thinking in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (e.g. a world in which I have not drunk the hot coffee), hence the name "counterfactual".
The ability to think in counterfactuals makes us humans so smart compared to other animals.

In interpretable machine learning, counterfactual explanations can be used to explain predictions of individual instances.
The "event" is the predicted outcome of an instance, the "causes" are the particular feature values of this instance that were input to the model and "caused" a certain prediction.
Displayed as a graph, the relationship between the inputs and the prediction is very simple:
The feature values cause the prediction.

```{r ml-graph-cf, fig.cap = "The causal relationships between inputs of a machine learning model and the predictions, when the model is merely seen as a black box. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data).", out.width=500}
knitr::include_graphics("images/graph.jpg")
```

Even if in reality the relationship between the inputs and the outcome to be predicted might not be causal, we can see the inputs of a model as the cause of the prediction.

Given this simple graph, it is easy to see how we can simulate counterfactuals for predictions of machine learning models:
We simply change the feature values of an instance before making the predictions and we analyze how the prediction changes.
We are interested in scenarios in which the prediction changes in a relevant way, like a flip in predicted class (e.g. credit application accepted or rejected) or in which the prediction reaches a certain threshold (e.g. the probability for cancer reaches 10%).
**A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.**

There are both, model-agnostic and model-specific, counterfactual explanation methods but throughout this chapter we want to focus model-agnostic methods which only work with the model inputs and output.
These methods would also feel at home in the [model-agnostic chapter](#agnostic), since the interpretation can be expressed as a summary of the differences in feature values ("change features A and B to change the prediction").
But a counterfactual explanation is itself a new instance, so it lives in this chapter ("starting from instance X, change A and B to get a counterfactual instance").
Unlike [prototypes](#proto), counterfactuals do not have to be actual instances from the training data, but can be a new combination of feature values.

Before discussing how to create counterfactuals, I would like to discuss some use cases for counterfactuals and how a good counterfactual explanation looks like.

In this first example, Peter applies for a loan and gets rejected by the (machine learning powered) banking software.
He wonders why his application was rejected and how he might improve his chances to get a loan.
The question of "why" can be formulated as a counterfactual:
What is the smallest change to the features (income, number of credit cards, age, ...) that would change the prediction from rejected to approved?
One possible answer could be:
If Peter would earn 10,000 Euro more per year, he would get the loan.
Or if Peter had fewer credit cards and had not defaulted on a loan 5 years ago, he would get the loan.
Peter will never know the reasons for the rejection, as the bank has no interest in transparency, but that is another story.

In our second example we want to explain a model that predicts a continuous outcome with counterfactual explanations.
Anna wants to rent out her apartment, but she is not sure how much to charge for it, so she decides to train a machine learning model to predict the rent.
Of course, since Anna is a data scientist, that is how she solves her problems.
After entering all the details about size, location, whether pets are allowed and so on, the model tells her that she can charge 900 Euro.
She expected 1000 Euro or more, but she trusts her model and decides to play with the feature values of the apartment to see how she can improve the value of the apartment.
She finds out that the apartment could be rented out for over 1000 Euro, if it were 15 m^2^ larger.
Interesting, but non-actionable knowledge, because she cannot enlarge her apartment.
Finally, by tweaking only the feature values under her control (built-in kitchen yes/no, pets allowed yes/no, type of floor, etc.), she finds out that if she allows pets and installs windows with better insulation, she can charge 1000 Euro.
Anna had intuitively worked with counterfactuals to change the outcome.

Counterfactuals are [human-friendly explanations](#good-explanation), because they are contrastive to the current instance and because they are selective, meaning they usually focus on a small number of feature changes.
But counterfactuals suffer from the 'Rashomon effect'. 
Rashomon is a Japanese movie in which the murder of a Samurai is told by different people.
Each of the stories explains the outcome equally well, but the stories contradict each other. 
The same can also happen with counterfactuals, since there are usually multiple different counterfactual explanations.
Each counterfactual tells a different "story" of how a certain outcome was reached.
One counterfactual might say to change feature A, the other counterfactual might say to leave A the same but change feature B, which is a contradiction.
This issue of multiple truths can be addressed either by reporting all counterfactual explanations or by having a criterion to evaluate counterfactuals and select the best one. 

Speaking of criteria, how do we define a good counterfactual explanation?
First, the user of a counterfactual explanation defines a relevant change in the prediction of an instance (= the alternative reality), so an obvious first requirement is that **a counterfactual instance produces the predefined prediction as closely as possible**.
It is not always possible to match the predefined output exactly.
In a classification setting with two classes, a rare class and a frequent class, the model could always classify an instance as the frequent class.
Changing the feature values so that the predicted label would flip from the common class to the rare class might be impossible.
We therefore want to relax the requirement that the predicted output of the counterfactual must correspond exactly to the defined outcome.
In the classification example, we could look for a counterfactual where the predicted probability of the rare class is increased to 10% instead of the current 2%.
The question then is, what are the minimum changes to the features so that the predicted probability changes from 2% to 10% (or close to 10%)?
Another quality criterion is that **a counterfactual should be as similar as possible to the instance regarding feature values**.
This requires a distance measure between two instances, for example the Manhattan Distance or the Gower distance if we have both discrete and continuous features.
The counterfactual should not only be close to the original instance, but should also **change as few features as possible**.
This can be achieved by selecting an appropriate  distance measure like the L-0 norm which counts the number of feature changes. 
The last requirement is that **a counterfactual instance should have feature values that are likely**. 
It would not make sense to generate a counterfactual explanation for the rent example where the size of an apartment is negative or the number of rooms is set to 200.
It is even better when the counterfactual is likely according to the joint distribution of the data, e.g. an apartment with 10 rooms and 20 m^2^ should not be regarded as counterfactual explanation. 
Ideally, if the number of square metres should be increased, an increase in the number of rooms is also suggested. 

### Generating Counterfactual Explanations

A simple and naive approach to generating counterfactual explanations is searching by trial and error.
This approach involves randomly changing feature values of the instance of interest and stopping when the desired output is predicted.
Like the example where Anna tried to find a version of her apartment for which she could charge more rent.
But there are better approaches than trial and error.
First, we define a loss function based on the criteria mentioned above. It takes as input the instance of interest, a counterfactual and the desired (counterfactual) outcome. 
Then, we can optimize the loss directly with an optimization algorithm. 
Many methods proceed in this way but differ in their definition of the loss function and optimization method. 

In this section, I will focus on two of them: first, the one by Wachter et. al (2017)[^wachter] who introduced counterfactual explanation as an interpretation method and, second, the one by Dandl et al. (2020)[^dandl] that takes into account all four criteria mentioned above. 


#### Method by Wachter et al. 
Wachter et al. suggest minimizing the following loss.


$$L(x,x^\prime,y^\prime,\lambda)=\lambda\cdot(\hat{f}(x^\prime)-y^\prime)^2+d(x,x^\prime)$$

The first term is the quadratic distance between the model prediction for the counterfactual x' and the desired outcome y', which the user must define in advance.
The second term is the distance d between the instance x to be explained and the counterfactual x', but more about this later.
The loss measures how far the predicted outcome of the counterfactual is from the predefined outcome and how far the counterfactual is from the instance of interest.
The parameter $\lambda$ balances the distance in prediction (first term) against the distance in feature values (second term).
The loss is solved for a given $\lambda$ and returns a counterfactual $x'$.
A higher value of $\lambda$ means that we prefer counterfactuals that come close to the desired outcome $y'$, a lower value means that we prefer counterfactuals $x'$ that are very similar to x in the feature values.
If $\lambda$ is very large, the instance with the prediction that comes closest to $y'$ will be selected, regardless how far it is away from $x$.
Ultimately, the user must decide how to balance the requirement that the prediction for the counterfactual matches the desired outcome with the requirement that the counterfactual is similar to $x$.
The authors of the method suggest instead of selecting a value for $\lambda$ to select a tolerance $\epsilon$ for how far away the prediction of the counterfactual instance is allowed to be from $y'$.
This constraint can be written as:

$$|\hat{f}(x^\prime)-y^\prime|\leq\epsilon$$

To minimize this loss function, any suitable optimization algorithm can be used, e.g. Nelder-Mead.
If you have access to the gradients of the machine learning model, you can use gradient-based methods like ADAM.
The instance $x$ to be explained, the desired output $y'$ and the tolerance parameter $\epsilon$ must be set in advance.
The loss function is minimized for $x'$ and the (locally) optimal counterfactual $x'$ returned while increasing $\lambda$ until a sufficiently close solution is found (= within the tolerance parameter).

$$\arg\min_{x^\prime}\max_{\lambda}L(x,x^\prime,y^\prime,\lambda)$$

The function $d$ for measuring the distance between instance x and counterfactual $x'$ is the Manhattan distance weighted feature-wise with the inverse median absolute deviation (MAD).

$$d(x,x^\prime)=\sum_{j=1}^p\frac{|x_j-x^\prime_j|}{MAD_j}$$

The total distance is the sum of all $p$ feature-wise distances, that is, the absolute differences of feature values between instance x and counterfactual $x'$.
The feature-wise distances are scaled by the inverse of the median absolute deviation of feature $j$ over the dataset defined as:


$$MAD_j=\text{median}_{i\in{}\{1,\ldots,n\}}(|x_{i,j}-\text{median}_{l\in{}\{1,\ldots,n\}}(x_{l,j})|)$$

The median of a vector is the value at which half of the vector values are greater and the other half smaller.
The MAD is the equivalent of the variance of a feature, but instead of using the mean as the center and summing over the square distances, we use the median as the center and sum over the absolute distances.
The proposed distance function has the advantage over the Euclidean distance that it introduces sparsity.
This means that two points are closer to each other when fewer features are different.
And it is more robust to outliers.
Scaling with the MAD is necessary to bring all the features to the same scale -- it should not matter whether you measure the size of an apartment in square meters or square feet.

The recipe for producing the counterfactuals is simple:

1. Select an instance x to be explained, the desired outcome $y'$, a tolerance $\epsilon$ and a (low) initial value for $\lambda$.
1. Sample a random instance as initial counterfactual.
1. Optimize the loss with the initially sampled counterfactual as starting point.
1. While $|\hat{f}(x^\prime)-y^\prime|>\epsilon$:
    - Increase $\lambda$.
    - Optimize the loss with the current counterfactual as starting point.
    - Return the counterfactual that minimizes the loss.
1. Repeat steps 2-4 and return the list of counterfactuals or the one that minimizes the loss.

Note that proposed method has three major disadvantages: 

* It **only takes our first and second criteria into account** not the last two ("produce counterfactuals with only a few feature changes and likely feature values").
* It does **not handle categorical features** with many different levels well. 
The authors of the method suggested running the method separately for each combination of feature values of the categorical features, but this will lead to a combinatorial explosion if you have multiple categorical features with many values.
For example, 6 categorical features with 10 unique levels would mean 1 million runs. 
* **Choosing $\lambda$ is not straight-forward** and highly depends on user 
preferences. 

<!-- A solution for only categorical features was proposed by Martens et. al (2014)[^martens]. -->
<!-- A solution that handles both numerical and categorical variables with a principled way of generating perturbations for categorical variables is implemented in the Python package [Alibi](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html). -->
Let us now have a look on another approach overcoming these issues. 


#### Method by Dandl et al. 
Dandl et al. suggest to simultaneously minimize a four-objective loss 
$$  L(x,x',y',X^{obs})=\big(o_1(\hat{f}(x'),y'),\,o_2(x, x'),o_3(x,x'),o_4(x',X^{obs})\big) $$ 
First, note that compared to the loss of Wachter et al. in addition to $x$, $x'$ and $y'$ we also need $X^{obs}$, the observed/training data, as an input. 
This is necessary for the fourth term which reflects that our counterfactuals should have likely feature values. 
We can extract what likely means directly from the input data we used to train our model or from another observed dataset. 
Second, note that no balancing/weighting terms $\lambda$ appear in the loss because we do not collapse the four objectives 
$o_1$, $o_2$, $o_3$ and $o_4$ into a single objective. 
We will later see how we can optimize all four terms simultaneously. 

The first term $o_1$ reflects that the prediction of our counterfactual $x'$ should be as close as possible to our desired prediction $y'$.
We therefore want to minimize the distance between $\hat{f}(x')$ and $y'$ denoted as the Manhattan metric ($L_1$ norm): 

$$ o_1(\hat{f}(x'),Y') = \begin{cases}0&\text{if $\hat{f}(x') \in Y'$}\\
\inf \limits_{y'\in Y'}|\hat{f}(x')-y'|&\text{else}\end{cases}.$$

The second component $o_2$ reflects that our counterfactual should be as similar as possible to our instance $x$. 
It quantifies the distance between $x'$ and $x$ 
as the Gower distance \cite{gower71}:
\begin{equation*}
o_2(x, x') = \frac{1}{p}\sum_{j = 1}^{p} \delta_G(x_j, x'_j)\in [0, 1]
\end{equation*}
with $p$ being the number of features.
The value of $\delta_G$ depends on the feature type:
\begin{equation*}
\delta_G(x_j, x'_j) = 
\begin{cases}
\frac{1}{\widehat{R}_j}|x_j- x'_j| & \text{if $x_j$ is numerical} \\
\mathbb{I}_{x_j \neq x'_j} & \text{if $x_j$ is categorical}
\end{cases}
\end{equation*}
with $\widehat{R}_j$ as the value range of feature $j$, extracted from the observed dataset. 
Note that the Gower distance can handle both numeric and discrete features but does not 
count how many features were changed. 

Therefore, we count the number of features in a third objectives $o_3$ using the $L_0$ norm:
$$o_3(x,x')=||x-x'||_0=\sum_{j = 1}^{p}\mathbb{I}_{x'_j\neq x_j}.$$

The fourth objective reflects that we want likely feature combinations for our counterfactual. 
As an approximation for the likelihood, $o_4$ measures the average Gower distance between $x'$ and the nearest observed data point $x^{[1]}$. 
$$o_4(x',\textbf{X}^{obs})=\frac{1}{p}\sum_{j=1}^{p}\delta_G(x'_j,x^{[1]}_j)\in[0, 1]$$

How can we minimize these four losses simultaneously? We use the **Non-dominated Sorting Genetic Algorithm** or short NSGA-II. 
NSGA-II is a nature-inspired algorithm that applies Darwin's law of the "survival of the fittest". 
We denote the fitness of a counterfactual by its vector of objectives values $(o_1, o_2, o_3, o_4)$. 
The algorithm consists of four steps that are repeated until a stopping criterion is met. 
One repetition is also called generation. 
[Figure 6.2](#fig:nsgaII-cf) visualizes the four steps for one generation. 

```{r nsgaII-cf, fig.cap = "Visualization of one generation of the NSGA-II algorithm.", out.width=700}
knitr::include_graphics("images/cfexp-nsgaII.jpg")
```

In the first generation a group of counterfactual candidates is initialized by randomly changing some of the features compared to our instance $x$ to be explained. 
Sticking with above's credit example, one counterfactual could suggest to increase the income by € 30,000 while another one proposes to have no default in the last 5 years and a reduction in age by 10. 
All other features are equal to the value of $x$. 
Each candidate is then evaluated using the four objective functions of above and the best ones with minimal objective values are selected. 
They are pairwisely recombined to produce children that are pretty similar to them. 
In addition, we slightly mutate the feature values of the children to explore the whole feature space.

From the two resulting groups, the one including the parents and the one with children, we only want the best half using two sorting algorithms. 
The nondominated sorting algorithm sorts the candidates according to the objective values. 
The crowding distance sorting algorithm sorts the candidates with equally objective values according to their diversity. 
In the end, we select the most promising and/or most diverse half of the candidates. 

We use this set for the next generation and start again with the selection, recombination and mutation process. 
By repeating the steps over and over again we hopefully approach more and more promising candidates with lower objective values.

In the end we receive a whole set of solutions from which we can choose the ones we are most satisfied with or we can summarizes the most promising counterfactuals visually as we see in the following data example.  

### Examples

The example is based on the credit dataset example in Dandl et al. (2020).
The used German Credit Risk dataset is from a kaggle challenge. 

The authors trained a support vector machine (with radial basis kernel) to predict the probability that a costumer has a good credit risk. 
The corresponding dataset has 522 complete observations and nine features containing credit and customer information. 

The goal is to find counterfactual explanations for a customer with the following feature values 

|age|sex|job |housing|savings|checking|credit.amount|duration|purpose|
|---|---|----|-------------|--------|---------------|----------|----------|----|
|58  |female  |1|free|little|little |6143            |48     |car|

The SVM predicts a probability of 0.242 that the woman has a good credit risk. 
The counterfactuals should answer how the input features need to be changed, to get a predicted probability larger 0.5?

The following table shows the 10 best counterfactuals:

|age|sex|job |credit.amount|duration|$o_2$|$o_3$|$o_3$|$\hat{f}(x')$|
|---|---|----|-------------|--------|---------------|----------|----------|----|
|0  |0  |2   |0            |-20     |0.108          |2         |0.036     |0.501|
|0  |0  |2   |0            |-24     |0.114          |2         |0.029     |0.525|
|0  |0  |2   |0            |-22     |0.111          |2         |0.033     |0.513|
|-6 |0  |2   |0            |-24     |0.126          |3         |0.018     |0.505|
|-3 |0  |2   |0            |-24     |0.120          |3         |0.024     |0.515|
|-1 |0  |2   |0            |-24     |0.116          |3         |0.027     |0.522|
|-3 |male|1   |0            |-24     |0.195          |3         |0.012     |0.501|
|-6 |male|1   |0            |-25     |0.202          |3         |0.011     |0.501|
|-30|male|2   |0            |-24     |0.285          |4         |0.005     |0.590|
|-4 |male|1   |-1254        |-24     |0.204          |4         |0.002     |0.506|


The first 5 column contains the proposed feature changes (only features that are changed are displayed), the next 3 columns the objective values ($o_1$ is 0 in all cases) and the last colum displays the predicted probability. 

All counterfactuals suggest a reduction of the duration from 48 month to 23 to 28 month, some of them propose that the woman should become skilled (job = 2) instead of unskilled (job = 1). 
Some counterfactuals even suggest to change the gender from female to male which shows a gender bias of the model. 
This change is always accompanied by reducing the age between 1 and 30 years. 
We can also see that although some counterfactuals suggest changes to many features, these counterfactuals are the ones that are closest to the training data. 


### Advantages

**The interpretation of counterfactual explanations is very clear**.
If the feature values of an instance are changed according to the counterfactual, the prediction changes to the predefined prediction.
There are no additional assumptions and no magic in the background. 
This also means it is not as dangerous as methods like [LIME](#lime), where it is unclear how far we can extrapolate the local model for the interpretation.

The counterfactual method creates a new instance, but we can also summarize a counterfactual by reporting which feature values have changed.
This gives us **two options for reporting our results**.
You can either report the counterfactual instance or highlight which features have been changed between the instance of interest and the counterfactual instance.

The **counterfactual method does not require access to the data or the model**.
It only requires access to the model's prediction function, which would also work via a web API, for example.
This is attractive for companies which are audited by third parties or which are offering explanations for users without disclosing the model or data. 
A company has an interest in protecting model and data because of trade secrets or data protection reasons.
Counterfactual explanations offer a balance between explaining model predictions and protecting the interests of the model owner.

The method **works also with systems that do not use machine learning**.
We can create counterfactuals for any system that receives inputs and returns outputs.
The system that predicts apartment rents could also consist of handwritten rules, and counterfactual explanations would still work.

**The counterfactual explanation method is relatively easy to implement**, since it is essentially a loss function that can be optimized with standard optimizer libraries. 
Some additional details must be taken into account, such as limiting feature values to meaningful ranges (e.g. only positive apartment sizes).

### Disadvantages

**For each instance you will usually find multiple counterfactual explanations (Rashomon effect)**.
This is inconvenient -- most people prefer simple explanations over the complexity of the real world.
It is also a practical challenge.
Let us say we generated 23 counterfactual explanations for one instance. 
Are we reporting them all?
Only the best?
What if they are all relatively "good", but very different?
These questions must be answered anew for each project.
It can also be advantageous to have multiple counterfactual explanations, because then humans can select the ones that correspond to their previous knowledge.

There is **no guarantee that for a given tolerance $\epsilon$ a counterfactual instance is found**.
That is not necessarily the fault of the method, but rather depends on the data.

### Software and Alternatives {#example-software}

Counterfactuals explanations are implemented in the Python package [Alibi](https://github.com/SeldonIO/alibi). Authors of the package implement a [simple counterfactual method](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html) as well as an [extended method](https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html) that uses class prototypes to improve the interpretability and convergence of the algorithm outputs[^vanlooveren].

A very similar approach was proposed by Martens et. al (2014) for explaining document classifications.
In their work, they focus on explaining why a document was or was not classified as a particular class.
The difference to the method presented in this chapter is that Martens et. al (2014) focus on text classifiers, which have word occurrences as inputs.

An alternative way to search counterfactuals is the Growing Spheres algorithm by Laugel et. al (2017)[^spheres].
The method first draws a sphere around the point of interest, samples points within that sphere, checks whether one of the sampled points yields the desired prediction, contracts or expands the sphere accordingly until a (sparse) counterfactual is found and finally returned.
They do not use the word counterfactual in their paper, but the method is quite similar.
They also define a loss function that favors counterfactuals with as few changes in the feature values as possible.
Instead of directly optimizing the function, they suggest the above-mentioned search with spheres.

Anchors by Ribeiro et. al (2018)[^anchors] are the opposite of counterfactuals, see chapter about [Scoped Rules (Anchors)](#anchors).

[^martens]: Martens, David, and Foster Provost. "Explaining data-driven document classifications." (2014).

[^anchors]: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Anchors: High-precision model-agnostic explanations." AAAI Conference on Artificial Intelligence (2018).

[^spheres]: Laugel, Thibault, et al. "Inverse classification for comparison-based interpretability in machine learning." arXiv preprint arXiv:1712.08443 (2017).

[^wachter]: Wachter, Sandra, Brent Mittelstadt, and Chris Russell. "Counterfactual explanations without opening the black box: Automated decisions and the GDPR." (2017).

[^vanlooveren]: Van Looveren, Arnaud, and Janis Klaise. "Interpretable Counterfactual Explanations Guided by Prototypes."  arXiv preprint arXiv:1907.02584 (2019).

[^dandl]: Dandl, Susanne, Christoph Molnar, Martin Binder, Bernd Bischl. "Multi-Objective Counterfactual Explanations". In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham (2020).
