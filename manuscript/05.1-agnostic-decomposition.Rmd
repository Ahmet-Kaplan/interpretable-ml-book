```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```
<!--{pagebreak}-->


<!-- TODOs 

- Find good chapter position. Maybe after PDP and ALE?
- Research paper that cite  Generalized functional anova diagnostics for high-dimensional functions of dependent variables.
-->


## Functional Decompositon {#decomposition}

A prediction function is -- more or less -- simply a function that takes a high-dimensional input and outputs, in many cases, a 1-dimensional number.
Functional decomposition takes this high-dimensional function and splits it into lower-dimensional components.
Functional decomposition is both an interpretation method and mental model which immensily helps you to understand other interpretation methods.


<!-- Intuition -->
### Some Intuition
We start with a simple function that takes only two features as input and produces a 1-dimensional output, the prediction.
We can visualize what the function looks like using a 3-D plot or a heatmap:

```{r}
x1 = seq(from = -1, to = 1, length.out = 100)
x2 = seq(from = -1, to = 1, length.out = 100)
f = function(x1, x2){
   2 + exp(x1) - x2 + 0.5 * x1 * x2 
}

dat = expand.grid(x1 = x1, x2 = x2)
dat$y = f(dat$x1, dat$x2)
# mean(dat$y)
p = ggplot(dat, aes(x = x1, y = x2, fill = y, z = y)) + geom_tile() + geom_contour()
p
```

This is a rather simple function to understand, so the use of decomposition might seem to much, but it is still useful.
And especially when we have inputs with higher dimensions it should become clear why functional deomposition is so powerful.
I did not show the formula that produced the function.
This is on purpose, as in the machine learning setup, we don't have a neat little formula for the prediction function, but in the case that it has a closed formula (e.g., random forests), it would be long and ugly.

Can we decompose this function into lower-dimensional components?
This would allows us to disentangle effects of the features and all their interactions with other features.
This would allow us to describe feature effects and importances. 
The functional decomposition allows us to split the function into it's components.
For a two-dimensional function f, which only depends on two input features: $f(x_1, x_2)$, we can split the function in the following way:

$$f(x_1, x_2) = f_0 + f_1(x_1) + f_2(x_2) + f_{1,2}(x_{1,2})$$

The component $f_0$ is the intercept, components $f_1$ and $f_2$ are the main effects of $x_1$ and $x_2$ and $f_{1,2}$ is the interaction effect between the two features.
The main effects tell us how each feature affects the prediction, independent of the other feature.
The interaction effect tells us what the effect of the features is together.
The intercept simply tells us what the prediction is when all feature effects are zero.

This formulation is basically a linear regression model, but where we include the interaction terms between all features (which of course makes it non-linear in the original feature space).

Let us look into the decomposition of the function above.
I just give it here, with explanations coming later.
We have the intercept with $f_0\sim3.18$
Here is the rest of the decomposition of the function above:



```{r}
```{r, out.height = 300, out.width = 900}
pred.fun = function(model = NULL, newdata){
  f(newdata$x1, newdata$x2)
}
pred = Predictor$new(predict.fun = pred.fun, data = dat, y = "y")
p1 = FeatureEffect$new(pred, feature = "x1",  method = "ale")$plot(rug = FALSE) +
  ggtitle(expression(paste(f[1], " main effect of ", X[1])))
p2 = FeatureEffect$new(pred, feature = "x2", method = "ale")$plot(rug = FALSE) +
  ggtitle(expression(paste(f[2], " main effect of ", X[2])))
interact = FeatureEffect$new(pred, feature = c("x1", "x2"), method = "ale")
p12 = interact$plot(rug = FALSE) + geom_contour(aes(z = .ale), color = "black") +
  scale_fill_continuous("value", low = "blue",  high = "yellow") +
  ggtitle(expression(paste(f[12], " interaction between ", X[1], " and ", X[2])))
p1 + p2 + p12
```


TODO: centered PDPs for funtion above.

Before we go into details how exactly this was computed, and whether this is the only solution (spoiler: no), we head deeper into the mathematics.

### Deeper Into the Mathematics

The example was two-dimensional.
But let us talk about p-dimensional functions: $f: \mathbb{R}^p \mapsto \mathbb{R}^1$.
How does a composition of this function look like?
Here it is:

$$f(x) = f_0 + f_1(x_1) + \ldots + f_p(x_p) + f_{1,2}(x_1, x_2) + \ldots + f_{p-1, p}(x_{p - 1}, x_p) + \ldots + f_{1,\ldots,p}(x_1, \ldots, x_p)$$

Okay that's really ugly and long, we shorten it a bit.
We can make it more precise and shorter by indexing all possible subsets of feature combinations: $S\subseteq\{1,\ldots,p\}$.
This contains all main effects, and all interactions, and also the empty set, which we need to define the intercept.
The the function $f$ can be decomposed as:

$$f(x) = \sum_{S\subseteq\{1,\ldots,p\}} f_S(x_S)$$

Here, $x_S$ is the vector of features in the index set $S$.



### How to estimate this?

We start with the simple version first, where the features are independent of each other, and later think about the more difficult case when they have some dependence structures.

The big problem with a functional decomposition is that it is quite arbitrary if we don't pose any limitations on how each of the components look like.
I can give you some food for thought.
We start with the minimal requirement that summing up our components (the $f_S$'s) actually sums up to the function $f$.
That means that no matter what input we put into $f$ and the components, the outcome is equal.
Let's say you have a 3-dimensional function.
It actually does not matter how this function look like, but the following decomposition would be valid:
$f_0$ is 0.12.
$f_1 = 2 \cdot x_1$ plus the number of shoes you own.
$f_2, $f_3$, $f_{1,2}$, $f_{2,3}, f_{1,3}$ are all zero.
And finally to make this trick work, I define:

$$f_{1,2,3} = f(x) - \sum_{S\subset\{1,\ldots,p\}} f_S(x_S)$$

Not very meaningful, and quite deceptive if you would present this as the interpretation of your model.
How to we prevent this ambiguity?

Enter the functional ANOVA

### functional ANOVA

The functional ANOVA was proposed by Hooker (2004)[^fanova].
The requirement is that the model prediction function f is square integrable.
The decomposition is as any decomposition:

$$f(x) = \sum_{S\subseteq\{1,\ldots,p\}} f_S(x_S)$$

Hooker (2004) proposes to estimate the individual components as:

$$f_S(x) = \int_{X_{-S}} \left( f(x) - \sum_{V \subset S} f_V(x)\right) d X_{-S})$$

Ok, let's take this thing apart.
Instead we can also write:

$$f_S(x) = \int_{X_{-S}} \left( f(x)\right) d X_{-S}) - \int_{X_{-S}} \left(\sum_{V \subset S} \right) d X_{-S})$$

The first part is the integral over the prediction function, with respect of the features that are not in the set.
This the same as the expectation of the function when we integrate out features $X_{-S}$, and pretending that all features follow a uniform distribution.
The second part are all the lower dimensional components, so we apply some kind of centering.
For example if $S=\{1,2\}$, meaning we look at the interaction effect of the first two features, and let's say there are, in total, 4 features.
Since the formula is recursive in the sense that to compute higher order interactions, you have to also compute all lower-order interactions, we start with the lowest order, namely the $f_0$ component which is a constant.
For $f_0$, the subset $S=\{\emptyset\}$ and therefore -S contains all features $XS.
Therefore we get:

$$f_S(x) = \int_{X} f(x) d X)$$

This is simply the prediction function where we integrated over all features.
And it can also be seen as the expectation of the function, but when we assume that all features are uniformly distributed.
Now that we have $f_0$, we can get the formula for $f_1$:

$$f_1(x) = \int_{X_{-1}} \left( f(x) - f_0\right) d X_{-S})$$

The formula for $f_2(x)$ is equivalent.

The the $V\subset{}S$ are $\{\emptyset,1,2\} and the formula becomes:

$$f_{1,2}(x) = \int{\{3,4\}} \left( f(x) - (f_0(x) + f_1(x) + f_2(x))\right) d X_{3},X_4$$

This example shows how each higher order effect is defined by integrating over all other features, but also be removing all the lower-order effects that are subsets of the higher-order effects.

Hooker (2004) showed that this fullfills a few desirable axioms:

- Zero Means: $\int{}f_S(x_S)dX_s=0$ for each $\S\neq\emptyset$.
- Orthogonality: $\int{}f_S(x_S)f_V(x_v)dX=0$ for $S\neq{}V$
- Variance Decomposition: Let $\sigma^2_{f}=\int{}f(x)^2dX$, then 
  $$ \sigma^2(f) = \sum_{S \subseteq P} \sigma^2_S(f_S),$$
  where P is the set of all features.

The zero means axiom implies that all effects or interactions are centered around zero.
The orthogonality axiom says that any two components do not share information, meaning that, for example, the first order effect of feature $X_1$ and the interaction term of $X_{1}$ and $X_2$ are not correlated.
The variance decomposition allows us to split the variance of the function $f$ among the components, and guarantees that it really adds up in the end.

TODO: CITE HOOKER and the axioms
CITE: Stein

Formula and how to interpret it and what to do with the individual parts

Axioms


Actually solved with (centered) PDPs


### functional ANOVA for dependent features

New axioms

Definition

Estimation



### Function Decomposition as Method

There are different ways to decompose a function.

For functions such as the [linear regression model](#limo) or [generalized additive models (GAMs)](#extended-lm), we already get a decomposition for free.

References:

- Stein
- Hooker fANOVA 1 and 2
- [Accumulated Local Effects Plots](#ale)
- See what Hooker cites
- See newer stuff (HDMR and whatnot)


### Viewing Other Methods as Decompositions

You might want to come back to this chapter again if you have a good grasp on some of the othre methods.

First on a high level:
Feature effects are direct visualizations of the individual components.
However we have to distinguish between total effect and isolated effect for the higher-order feature effects.
PDP is total effect
ALE is individual effect
If you remove lower effects from PDP, you get fANOVA, at least for indepdentn feature case.

PDP is a direct decomposition, but with additional intercept difference.
ALE is a decomposition.
For permutation feature importance,
Methods such as SHApley and co only describe a prediction with 1-dimensional effects.
What happened with the interaction terms? They are divided among the individual effects.
What happened in PFI with the interactions? They are alos divided among individual effects.

The SHAP interaction plots can also be better understood through decompositions.



### Advantages

A rigorous way of thinking about high-dimensional functions.

Allows to understand most other interpretation methods much better.

Separation and attribution of effects.


### Disadvantages

When output high-dimensional, than not as simple.

Makes little sense for images and text.

No clear superior way for the axioms. 

Estimating higher effect components always difficult and no good way to visualize.

[^fanova]: Hooker, Giles. "Discovering additive structure in black box functions." Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. 2004.

[^fanova2]: Hooker, Giles. "Generalized functional anova diagnostics for high-dimensional functions of dependent variables." Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.

[^ale]: Apley, Daniel W., and Jingyu Zhu. "Visualizing the effects of predictor variables in black box supervised learning models." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.

